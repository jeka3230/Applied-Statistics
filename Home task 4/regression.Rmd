---
title: "Прогнозирование"
author: "Евгений Смирнов, 274 группа"
date: "May 12, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Задание
- Визуализировать данные, проанализировать распределения признака (оценить необходимость трансформации), оценить наличие выбросов, преобразования для снятия календарных эффектов;
- Настроить модели ARIMA:
  - выбрать модель руками на основании анализа ACF/PACF, произвести поиск наилучшей модели в окрестности, выполнить анализ остатков
  - автоматический способом подбора модели, проверить её соответствие особенностями ряда, сделать анализ остатков;
- Настроить модель экспоненциального сглаживания автоматическим методом подбора модели, проверить её соответствие особенностям ряда, выполнить корректировку, произвести анализ остатков;
- Выполнить визуальный анализ, при необходимости — формальную проверку наличия структурных изменений в моделях;
- Сравнить и выбрать лучшую модель по критерию Диболда-Мариано;


```{r warning=FALSE, fig.height=5.5, fig.width=10, , message=FALSE}
library(forecast)
library(tseries)
library(lmtest)
library(Hmisc)
data = read.csv("lenex-corporation-shipment-of-ra.csv", header = T, stringsAsFactors = F)
names(data)[1] <- "Date"
names(data)[2] <- "Value"
xname <- "Lenex corporation: shipment of radios"

data$Value <- as.numeric(data$Value)
data$Date <- as.Date(as.yearmon(data$Date, format="%Y-%m"))
tSeries <- ts(data = data$Value, start = as.numeric(c(format(data$Date[1], "%Y"), format(data$Date[1], "%m"))), freq = 12)
plot(tSeries, type="l", ylab=xname, col="red")
grid()

trainSeries <- window(tSeries, end=c(1976,01))
testSeries  <- window(tSeries, start=c(1976,2))
D = 36
```

Попробуем поделить на число дней в месяце:

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
plot(tSeries / monthDays(as.Date(time(tSeries))), type="l", ylab=xname, col="red")
grid()
```

Ряд не стал более регулярным, так что вернёмся к исходным данным.

STL-декомпозиция ряда:

```{r, echo=TRUE, fig.height=8, fig.width=10}
plot(stl(tSeries, s.window="periodic"))
```

Оптимальное преобразование Бокса-Кокса и результат его применения: 

```{r, echo=TRUE, fig.width=10, fig.height=8}
par(mfrow=c(2,1))
plot(tSeries, ylab="Original series", xlab="", col="red")
grid()

LambdaOpt <- BoxCox.lambda(tSeries)
plot(BoxCox(tSeries, LambdaOpt), ylab="Transformed series", xlab="", col="red")
title(main=toString(round(LambdaOpt, 3)))
grid()
```

Преобразование стабилизирует дисперсию, будем его использовать.

```{r}
y<-BoxCox(tSeries, LambdaOpt)
n<- length(y)
m<- ets(y)
k<- length(m$par)
rss <- sum(m$residuals^2)
m1<- ets(y[1:109])
k1<- length(m1$par)
rss1 <- sum(m1$residuals^2)
m2<- ets(y[110:n])
k2<- (m2$aic + 2*m2$loglik)/2
rss2 <- sum(m2$residuals^2)
f <- ((rss-rss1-rss2)/(k1+k2-k)) / ((rss1-rss2)/(n-k1-k2))
pf(f, k1+k2-k, n-k1-k2)
```



## Прогноз ETS
```{r, echo=TRUE}
fit.ets <- ets(tSeries, lambda = LambdaOpt)
print(fit.ets)
```

Настроив выбранную модель на обучающей выборке, посчитаем её качество на тестовой:
```{r, echo=FALSE, fig.height=5.5, fig.width=10}
fitShort <- ets(trainSeries, model="ANA")
fc       <- forecast(fitShort, h=D)
accuracy(fc, testSeries)
plot(forecast(fitShort, h=D), ylab=xname, xlab="Year")
lines(tSeries, col="red")
```

Остатки:
```{r, echo=TRUE, fig.height=8, fig.width=10}
tsdisplay(residuals(fit.ets))
```

Достигаемые уровни значимости критерия Льюнга-Бокса для них:
```{r, echo=TRUE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(residuals(fit.ets), lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

Остатки некоррелированы.

Q-Q plot и гистограмма для остатков:
```{r, echo=TRUE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(residuals(fit.ets))
qqline(residuals(fit.ets), col="red")
hist(residuals(fit.ets))
```

У распределения нет тяжёлых хвостов. 

Гипотеза           | Критерий      | Результат проверки | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность       | Шапиро-Уилка  | не отвергается        | `r shapiro.test(residuals(fit.ets))$p.value`
Несмещённость      | Уилкоксона    | не отвергается     | `r wilcox.test(residuals(fit.ets))$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(residuals(fit.ets))$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается     | `r bptest(residuals(fit.ets) ~ c(1:length(residuals(fit.ets))))$p.value`

```{r echo=TRUE}
res.ets  <- tSeries - fit.ets$fitted
```


## ARIMA
### Ручной подбор модели

Исходный ряд нестационарен (p<`r kpss.test(BoxCox(tSeries, LambdaOpt))$p.value`, критерий KPSS); сделаем сезонное дифференцирование: 

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
plot(diff(BoxCox(tSeries, LambdaOpt), 12), type="l", col="red")
grid()
```
Для полученного ряда гипотеза стационарности не отвергается (p = `r kpss.test(diff(BoxCox(tSeries, LambdaOpt), 12))$p.value`)

Посмотрим на ACF и PACF полученного продифференцированного ряда:

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
acf(diff(BoxCox(tSeries, LambdaOpt), 12), lag.max=5*12, main="")
pacf(diff(BoxCox(tSeries, LambdaOpt), 12), lag.max=5*12, main="")
```

Будем искать модель в окрестности ARIMA(2,0,6)(2,1,1)$_{12}$

Модель                     | AICc
-------------------------- | ------------
ARIMA(2,0,6)(2,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(2,1,1), lambda=LambdaOpt)$aicc`
ARIMA(1,0,6)(2,1,1)$_{12}$ | `r Arima(tSeries, order=c(1,0,6), seasonal=c(2,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,5)(2,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,5), seasonal=c(2,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(2,1,0)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(2,1,0), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(1,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,7)(2,1,0)$_{12}$ | `r Arima(tSeries, order=c(2,0,7), seasonal=c(2,1,0), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(2,1,2)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(2,1,2), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(3,1,0)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(3,1,0), lambda=LambdaOpt)$aicc`
ARIMA(1,0,5)(2,1,1)$_{12}$ | `r Arima(tSeries, order=c(1,0,5), seasonal=c(2,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(1,1,0)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(1,1,0), lambda=LambdaOpt)$aicc`
ARIMA(2,0,5)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,5), seasonal=c(1,1,1), lambda=LambdaOpt)$aicc`
ARIMA(1,0,6)(1,1,1)$_{12}$ | `r Arima(tSeries, order=c(1,0,6), seasonal=c(1,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(0,1,1), lambda=LambdaOpt)$aicc`
ARIMA(3,0,6)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(3,0,6), seasonal=c(0,1,0), lambda=LambdaOpt)$aicc`
ARIMA(2,0,7)(0,1,1)$_{12}$ | `r Arima(tSeries, order=c(2,0,7), seasonal=c(0,1,1), lambda=LambdaOpt)$aicc`
ARIMA(2,0,6)(0,1,2)$_{12}$ | `r Arima(tSeries, order=c(2,0,6), seasonal=c(0,1,2), lambda=LambdaOpt)$aicc`

Возьмём модель ARIMA(1,0,6)(1,1,1)$_{12}$, как модель с наименьшим AIC.

```{r, echo=TRUE, fig.height=4.5, fig.width=10}
fit <- Arima(tSeries, order=c(1,0,6), seasonal=c(1,1,1), lambda=LambdaOpt)
res <- residuals(fit)
plot(res)
```

Видно, что в начале ряда остатки не определены, что логично, поскольку модель сезонная. Отрежем начало ряда остатков и проанализируем их: 

```{r, echo=TRUE, fig.height=8, fig.width=10}
res <- res[-c(1:12)]
tsdisplay(res)
```

Достигаемые уровни значимости критерия Льюнга-Бокса для остатков: 

```{r, echo=TRUE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

Q-Q plot и гистограмма: 

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res)
qqline(res, col="red")
hist(res)
```

Гипотеза           | Критерий      | Результат проверки	 | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность      | Шапиро-Уилка  | не отвергается        | `r shapiro.test(res)$p.value`
Несмещённость      | Уилкоксона    | не отвергается     | `r wilcox.test(res)$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(res)$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается        | `r bptest(res ~ c(1:length(res)))$p.value`

Настроив выбранную модель на обучающей выборке, посчитаем её качество на тестовой:
```{r, echo=TRUE, fig.height=5.5, fig.width=10}
fitShort <- Arima(trainSeries, order=c(1,0,6), seasonal=c(1,1,1), lambda=LambdaOpt)
fc       <- forecast(fitShort, h=D)
accuracy(fc, testSeries)
plot(forecast(fitShort, h=D), ylab=xname, xlab="Time")
lines(tSeries, col="red")
```

### Автоматический подбор модели
Применим функцию auto.arima:
```{r, echo=TRUE, warning=FALSE, message=FALSE}
fit.auto <- auto.arima(tSeries, lambda = LambdaOpt)
fit.auto
```
Предлагается модель ARIMA(1,1,0)(2,0,0)$_{12}$. Её AICc выше, чем у модели, подобранной вручную. Посмотрим на её остатки:
```{r, echo=TRUE, fig.height=4.5, fig.width=10}
res.auto <- residuals(fit.auto)
plot(res.auto)
```

 Посмотрим на её остатки: 
 
```{r, echo=TRUE, fig.height=4.5, fig.width=10}
res.auto <- residuals(fit.auto)
plot(res.auto)
```
Отрежем первые 13 отсчётов и продолжим анализ:
```{r, echo=TRUE, fig.height=8, fig.width=10}
res.auto <- res.auto[-c(1:13)]
tsdisplay(res.auto)
```

```{r, echo=TRUE}
p <- rep(0, 1, frequency(tSeries)*3)
for (i in 1:length(p)){
  p[i] <- Box.test(res.auto, lag=i, type = "Ljung-Box")$p.value
}
plot(p, xlab="Lag", ylab="P-value", ylim=c(0,1))
abline(h = 0.05, lty = 2, col = "blue")
```

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
par(mfrow=c(1,2))
qqnorm(res.auto)
qqline(res.auto, col="red")
hist(res.auto)
```

Гипотеза           | Критерий      | Результат проверки	 | Достигаемый уровень значимости
------------------ | ------------- | ------------------ | ------------------------------
Нормальность      | Шапиро-Уилка  | не отвергается        | `r shapiro.test(res.auto)$p.value`
Несмещённость      | Уилкоксона    | не отвергается     | `r wilcox.test(res.auto)$p.value`
Стационарность     | KPSS          | не отвергается     | `r kpss.test(res.auto)$p.value`
Гомоскедастичность | Бройша-Пагана | не отвергается        | `r bptest(res.auto ~ c(1:length(res.auto)))$p.value`

Остатки автоматической модели не лучше, а её AICc больше, так что остановимся на модели, подобранной вручную.

Настроив выбранную модель на обучающей выборке, посчитаем её качество на тестовой:
```{r, echo=TRUE, fig.height=5.5, fig.width=10}
fitShort <- Arima(trainSeries, order=c(1,1,0), seasonal=c(2,0,0), lambda=LambdaOpt)
fc       <- forecast(fitShort, h=D)
accuracy(fc, testSeries)
plot(forecast(fitShort, h=D), ylab=xname, xlab="Time")
lines(tSeries, col="red")
```

Сравним остатки двух версий аримы, одинаково обрезав их начало так, чтобы у обоих методов они были правильно определены:
```{r, echo=TRUE, fig.height=8, fig.width=8}
res      <- (tSeries - fitted(fit))[-c(1:13)]
res.auto <- (tSeries - fitted(fit.auto))[-c(1:13)]

plot(res, res.auto, xlim=c(min(res, res.auto), max(res, res.auto)), ylim=c(min(res, res.auto), max(res, res.auto)), 
     xlab = "Residuals of manually found model", ylab="Residuals of auto.arima model")
grid()
lines(c(min(res, res.auto), max(res, res.auto))*2, c(min(res, res.auto), max(res, res.auto))*2, col="red")

dm.test(res, res.auto)
```

В целом автоматическая модель сложнее, её остатки не лучше, AICc — больше, и ошибка на тесте больше, так что остановимся на модели, подобранной вручную.

## Итоговое сравнение
Сравним остатки лучших моделей ARIMA и ETS, одинаково обрезав их начало так, чтобы у обоих методов они были правильно определены:
```{r fig.width=8, fig.height=8, echo=TRUE}
res.ets <- (tSeries - fitted(fit.ets))[-c(1:13)]

plot(res, res.ets, 
     xlab="Residuals, best ARIMA",
     ylab="Residuals, best ETS",
     xlim=c(min(c(res, res.ets), na.rm=T), max(c(res, res.ets), na.rm=T)),
     ylim=c(min(c(res, res.ets), na.rm=T), max(c(res, res.ets), na.rm=T)))
 lines(c(min(c(res, res.ets), na.rm=T), max(c(res, res.ets), na.rm=T)), c(min(c(res, res.ets), na.rm=T), max(c(res, res.ets), na.rm=T)), col="red")

dm.test(res, res.ets)
dm.test(res, res.ets, alternative = "less")
```
Согласно критерию Диболда-Мариано, прогнозы метода ARIMA не хуже, её остатки больше, но анамальность меньше, чем у модели ETS, значит будем использовать модель ARIMA, настроенную в ручную.


### Финальный прогноз

```{r, echo=TRUE, fig.height=5.5, fig.width=10}
fl <- forecast(fit, h=D)
print(fl)
plot(fl, ylab=xname, xlab="Year", col="red")
```
